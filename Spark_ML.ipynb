{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Linear Regression**\n",
    "#### This lab covers a common supervised learning pipeline, using a subset of the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD). Our goal is to train some models to predict the release year or time period of a song given a set of audio features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The raw data is currently stored in text file.  We will start by storing this raw data in as an RDD, with each element of the RDD representing a data point as a comma-delimited string. Each string starts with the label (a year) followed by numerical audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2001.0,0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817',\n",
       " u'2001.0,0.854411946129,0.604124786151,0.593634078776,0.495885413963,0.266307830936,0.261472105188,0.506387076327,0.464453565511,0.665798573683,0.542968988766,0.58044428577,0.445219373624',\n",
       " u'2001.0,0.908982970575,0.632063159227,0.557428975183,0.498263761394,0.276396052336,0.312809861625,0.448530069406,0.448674249968,0.649791323916,0.489868662682,0.591908113534,0.4500023818']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "import sys\n",
    "sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "\n",
    "import hashlib\n",
    "hashed = lambda x: hashlib.sha1(str(x)).hexdigest()\n",
    "\n",
    "# load testing library\n",
    "from test_helper import Test\n",
    "import os.path\n",
    "%matplotlib inline\n",
    "\n",
    "numPartitions = 2\n",
    "rawData = sc.textFile('millionsong.txt', numPartitions)\n",
    "rawData.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "#### ** Check the data using `LabeledPoint` **\n",
    "#### In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Write the parsePoint function that takes as input a raw data point, parses it using Python's [unicode.split](https://docs.python.org/2/library/string.html#string.split) method, and returns a `LabeledPoint`.  Use this function to parse samplePoints by applying it to rawData.  Then print out the features and label for the first training point, using the `LabeledPoint.features` and `LabeledPoint.label` attributes. Finally, calculate the number features for this dataset.\n",
    "#### Note that `split()` can be called directly on a `unicode` or `str` object.  For example, `u'split,me'.split(',')` returns `[u'split', u'me']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# Here is a sample raw data point:\n",
    "# '2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419'\n",
    "# In this raw data point, 2001.0 is the label, and the remaining values are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parsePoint(line):\n",
    "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
    "\n",
    "    Args:\n",
    "        line (unicode): Comma separated unicode string where the first element is the label and the\n",
    "            remaining elements are features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features.\n",
    "    \"\"\"\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    \n",
    "    feats = <FILL IN>\n",
    "    \n",
    "    return LabeledPoint(feats[0], feats[1:])\n",
    "\n",
    "parsedSamplePoints = rawData.<FILL IN>\n",
    "firstPointFeatures = parsedSamplePoints.<FILL IN>\n",
    "firstPointLabel = parsedSamplePoints.<FILL IN>\n",
    "print firstPointFeatures, firstPointLabel\n",
    "\n",
    "d = len(firstPointFeatures)\n",
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Test.assertEqualsHashed(d, '7b52009b64fd0a2a49e6d8a939753077792b0554', 'incorrect number of features', 'Test 1 success')\n",
    "Test.assertEqualsHashed(firstPointLabel, 'adba49c03474bed805297a6455464fb0604db4d3', 'incorrect label for firstPointLabel', 'Test 2 success')\n",
    "expectedX0 = [0.8841,0.6105,0.6005,0.4747,0.2472,0.3573,0.3441,0.3396,0.6009,0.4257,0.6049,0.4192]\n",
    "Test.assertEqualsHashed((firstPointFeatures-expectedX0), '363289b928767d708220b62d1313f44725a305e9', \n",
    "                        'incorrect features for firstPointFeatures', 'Test 3 success')\n",
    "Test.assertEqualsHashed(d2, 'ac3478d69a3c81fa62e60f5c3696165a4e5e6ac4', 'incorrect number of Sample points', 'Test 4 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualization of Features**\n",
    "#### First we will load and setup the visualization library.  Then we will look at the raw features for 50 data points by generating a heatmap that visualizes each feature on a grey-scale and shows the variation of each feature across the 50 sample data points.  The features are all between 0 and 1, with values closer to 1 represented via darker shades of grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "sampleMorePoints = rawData.take(50)\n",
    "# You can uncomment the line below to see randomly selected features.  These will be randomly\n",
    "# selected each time you run the cell.  Note that you should run this cell with the line commented\n",
    "# out when answering the lab quiz questions.\n",
    "# sampleMorePoints = rawData.takeSample(False, 50)\n",
    "\n",
    "parsedSampleMorePoints = map(parsePoint, sampleMorePoints)\n",
    "dataValues = map(lambda lp: lp.features.toArray(), parsedSampleMorePoints)\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# generate layout and plot\n",
    "fig, ax = preparePlot(np.arange(.5, 11, 1), np.arange(.5, 49, 1), figsize=(8,7), hideLabels=True,\n",
    "                      gridColor='#eeeeee', gridWidth=1.1)\n",
    "image = plt.imshow(dataValues,interpolation='nearest', aspect='auto', cmap=cm.Greys)\n",
    "for x, y, s in zip(np.arange(-.125, 12, 1), np.repeat(-.75, 12), [str(x) for x in range(12)]):\n",
    "    plt.text(x, y, s, color='#999999', size='10')\n",
    "plt.text(4.7, -3, 'Feature', color='#999999', size='11'), ax.set_ylabel('Observation')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "#### **Find the range ** and ** Shift labels **\n",
    "#### Now let's examine the labels to find the range of song years.  To do this, first parse each element of the `rawData` RDD, and then find the smallest and largest labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "parsedDataInit = rawData.<FILL IN>\n",
    "onlyLabels = parsedDataInit.<FILL IN>\n",
    "minYear = <FILL IN>\n",
    "maxYear = <FILL IN>\n",
    "print maxYear, minYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataInit=(len(parsedDataInit.take(1)[0].features),hashed(parsedDataInit.map(lambda lp: lp.features[2]).sum()))\n",
    "yearRange = maxYear - minYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Find the range\n",
    "Test.assertEqualsHashed(DataInit, '7eeb056f9e35096daaaa6a8b814296693435242f', 'unexpected number of features in sample point', 'Test 5 success')\n",
    "Test.assertEqualsHashed(yearRange, '7ba8ed7dbc6518d461534ab671348bcbbeff6b42', 'incorrect label for firstPointLabel', 'Test 6 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we just saw, the labels are years in the 1900s and 2000s.  In learning problems, it is often natural to shift labels such that they start from zero.  Starting with `parsedDataInit`, create a new RDD consisting of `LabeledPoint` objects in which the labels are shifted such that smallest label equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "parsedData = parsedDataInit.<FILL IN>\n",
    "\n",
    "# Should be a LabeledPoint\n",
    "print type(parsedData.take(1)[0])\n",
    "# View the first point\n",
    "print '\\n{0}'.format(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oldSampleFeatures = parsedDataInit.take(1)[0].features\n",
    "newSampleFeatures = parsedData.take(1)[0].features\n",
    "DiffSampleFeatures=oldSampleFeatures-newSampleFeatures\n",
    "\n",
    "minYearNew = parsedData.map(lambda lp: lp.label).min()\n",
    "maxYearNew = parsedData.map(lambda lp: lp.label).max()\n",
    "yearNewRange=maxYearNew-minYearNew\n",
    "\n",
    "sumFeatTwo = parsedData.map(lambda lp: lp.features[2]).sum()\n",
    "print DiffSampleFeatures\n",
    "print yearNewRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Shift labels\n",
    "Test.assertEqualsHashed(DiffSampleFeatures, 'ec2e1dc7b5acfefe5ddd5683a5770daa571fa7af', 'new features do not match old features', 'Test 7 success')\n",
    "Test.assertEqualsHashed(yearNewRange, '7ba8ed7dbc6518d461534ab671348bcbbeff6b42', 'incorrect label for firstPointLabel', 'Test 8 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "#### **Training, validation, and test sets **\n",
    "#### We're almost done parsing our first dataset, and our final task involves split it into training, validation and test sets. Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets. Next, cache each of these RDDs, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset and verify that the sum of their sizes equals the 6724."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = parsedData.<FILL IN>\n",
    "parsedTrainData.<FILL IN>\n",
    "parsedValData.<FILL IN>\n",
    "parsedTestData.<FILL IN>\n",
    "nTrain = parsedTrainData.<FILL IN>\n",
    "nVal = parsedValData.<FILL IN>\n",
    "nTest = parsedTestData.<FILL IN>\n",
    "\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "print parsedData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumFeatTwo = (parsedTrainData\n",
    "              .map(lambda lp: lp.features[2])\n",
    "              .sum())\n",
    "sumFeatThree = (parsedValData\n",
    "                .map(lambda lp: lp.features[3])\n",
    "                .reduce(lambda x, y: x + y))\n",
    "sumFeatFour = (parsedTestData\n",
    "               .map(lambda lp: lp.features[4])\n",
    "               .reduce(lambda x, y: x + y))\n",
    "listSums=[sumFeatTwo, sumFeatThree, sumFeatFour]\n",
    "sumWhole=nTrain + nVal + nTest\n",
    "print listSums\n",
    "print sumWhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(sumWhole, '2cfc84ed2f2c864291ee146be109eb4a10e32a35', 'unexpected Train, Val, Test data set size', 'Test 9 success')\n",
    "Test.assertEqualsHashed(listSums, '6cbf9887b7256a2d4bd71f4f203ed18f2662ecd4', 'parsed Train, Val, Test data has unexpected values', 'Test 10 success')\n",
    "Test.assertEqualsHashed(parsedDataAll, '0019ff765b7fae3ae321cef02d908d445ac66006', 'parsedDatas have wrong number of partitions', 'Test 11 success')\n",
    "Test.assertEqualsHashed(len(parsedTrainData.take(1)[0].features), '7b52009b64fd0a2a49e6d8a939753077792b0554', \n",
    "                        'parsedTrainData havs wrong number of features', 'Test 12 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to traine the Linear Regression model\n",
    "### EXERCISE 4\n",
    "#### **MLlib's [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD)**\n",
    "####First use LinearRegressionWithSGD to train a model with L2 regularization and with an intercept.  This method returns a [LinearRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel).  Next, use the model's [weights](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.weights) and [intercept](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.intercept) attributes to print out the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData,iterations=numIters,step=alpha,miniBatchFraction=miniBatchFrac,\n",
    "                                           regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = <FILL IN>\n",
    "interceptLR1 = <FILL IN>\n",
    "print weightsLR1, interceptLR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST LinearRegressionWithSGD\n",
    "expectedIntercept = 13.3335907631\n",
    "expectedWeights = [16.682292427, 14.7439059559, -0.0935105608897, 6.22080088829, 4.01454261926, -3.30214858535,\n",
    "                   11.0403027232, 2.67190962854, 7.18925791279, 4.46093254586, 8.14950409475, 2.75135810882]\n",
    "Test.assertEqualsHashed(abs(interceptLR1-expectedIntercept)<1, '88b33e4e12f75ac8bf792aebde41f1a090f3a612', \n",
    "                        'incorrect value for interceptLR1', 'Test 13 success')\n",
    "Test.assertEqualsHashed(abs(weightsLR1[0]-expectedWeights[0])<1, '88b33e4e12f75ac8bf792aebde41f1a090f3a612', \n",
    "                        'incorrect value for weightsLR1', 'Test 14 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use the [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.predict) method to make a prediction on a sample point.  Pass the `features` from a `LabeledPoint` into the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "samplePoint = parsedTrainData.take(1)[0]\n",
    "samplePrediction = <FILL IN>\n",
    "print samplePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Predict\n",
    "Test.assertEqualsHashed(abs(samplePrediction - 56.8013380112)<1, '88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'incorrect value for samplePrediction', 'Test 15 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We naturally would like to see how well this performs.  We will use root mean squared error ([RMSE](http://en.wikipedia.org/wiki/Root-mean-square_deviation)) for evaluation purposes.  Implement a function to compute RMSE given an RDD of (label, prediction) tuples, and test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    <FILL IN>\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    <FILL IN>\n",
    "\n",
    "labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n",
    "# RMSE = sqrt[((3-1)^2 + (1-2)^2 + (2-2)^2) / 3] = 1.291\n",
    "exampleRMSE = calcRMSE(labelsAndPreds)\n",
    "print exampleRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Root mean squared error\n",
    "Test.assertEqualsHashed(squaredError(3, 1), '1b6453892473a467d07372d45eb05abc2031647a', 'incorrect definition of squaredError', 'Test 16 success')\n",
    "Test.assertEqualsHashed(exampleRMSE, '4b322e37f07576b9230316f90b84285b5d096941', 'incorrect value for exampleRMSE', 'Test 17 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next evaluate the accuracy of this model on the validation set.  Use the `predict()` method to create a `labelsAndPreds` RDD, and then use the `calcRMSE()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "labelsAndPreds = parsedValData.map(lambda x:(x.label,firstModel.<FILL IN>))\n",
    "rmseValLR1 = <FILL IN>\n",
    "\n",
    "print ('Validation RMSE: LR1 = {0:.11f}').format(rmseValLR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're already outperforming the baseline on the validation set by almost 2 years on average, but let's see if we can do better. Perform grid search to find a good regularization parameter.  Try `regParam` values `1e-10`, `1e-5`, and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "bestRMSE = rmseValLR1\n",
    "bestRegParam = reg\n",
    "bestModel = firstModel\n",
    "\n",
    "numIters = 500\n",
    "alpha = 1.0\n",
    "miniBatchFrac = 1.0\n",
    "for reg in <FILL IN>:\n",
    "    model = LinearRegressionWithSGD.train(parsedTrainData, numIters, alpha,\n",
    "                                          miniBatchFrac, regParam=reg,\n",
    "                                          regType='l2', intercept=True)\n",
    "    labelsAndPreds = parsedValData.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "    rmseValGrid = calcRMSE(labelsAndPreds)\n",
    "    print rmseValGrid\n",
    "\n",
    "    if rmseValGrid < bestRMSE:\n",
    "        bestRMSE = rmseValGrid\n",
    "        bestRegParam = reg\n",
    "        bestModel = model\n",
    "rmseValLRGrid = bestRMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Grid search\n",
    "Test.assertEqualsHashed((abs(rmseValLRGrid-17.017170071631259)<1,abs(rmseValLR1-19.691247341628721)<1), '6d968538694d0ff0b4e434242f8929869ce29fba', \n",
    "                        'incorrect values for rmseValLRGrid and rmseValLR1', 'Test 18 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes enough predict the time period. Let's traine some enouther model for period time prediction.\n",
    "\n",
    "### EVERCISE 5\n",
    "#### **MLlib's [DecisionTree](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=pyspark.mllib.tree#pyspark.mllib.tree.DecisionTree)** and **[RandomForest](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=pyspark.mllib.tree#pyspark.mllib.tree.RandomForest)** Classifier\n",
    "#### First we need replace year lable for time period label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def period_labels(in_line):\n",
    "    if minYear <= in_line.label < 1940.0:\n",
    "        in_line.label = 0.0\n",
    "    elif 1940.0 <= in_line.label < 1960.0:\n",
    "        in_line.label = 1.0\n",
    "    elif 1960.0 <= in_line.label < 1980.0:\n",
    "        in_line.label = 2.0\n",
    "    elif 1980.0 <= in_line.label < 2000.0:\n",
    "        in_line.label = 3.0\n",
    "    elif 2000.0 <= in_line.label <= maxYear:\n",
    "        in_line.label = 4.0\n",
    "    return in_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next create a new dataset with **period_labels()** function. Try to get it from rawData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "parsedData2 = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(parsedData2.map(lambda lp: lp.label).sum(), '0a64c49801579d52880487ca045606a2a37374a8', \n",
    "                        'Wrong period labels in Data', 'Test 19 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And split randomly  **parsedData2** to **parsedTrainData2** and **parsedTestData2** with **weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [.7, .3]\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "(parsedTrainData2, parsedTestData2) = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(parsedTrainData2.map(lambda lp: lp.label).sum(), '321d01504f5827edab56fde6fb493762f5d37521', \n",
    "                        'Incorect splitting', 'Test 20 success')\n",
    "Test.assertEqualsHashed(parsedTestData2.map(lambda lp: lp.label).sum(), '3ceee74eedb864ff868f9f0e67464a8f7a3abd05', \n",
    "                        'Incorect splitting', 'Test 21 success')\n",
    "Test.assertEqualsHashed(np.allclose(parsedData2.map(lambda lp: lp.label).sum(), \n",
    "                                    (parsedTrainData2.map(lambda lp: lp.label).sum()+parsedTestData2.map(lambda lp: lp.label).sum())), \n",
    "                                     '3ceee74eedb864ff868f9f0e67464a8f7a3abd05', 'Incorect splitting', 'Test 22 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Train DecisionTree model and predict it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = <FILL IN>.trainClassifier(<FILL IN>, numClasses=6, categoricalFeaturesInfo={},\n",
    "                                         impurity='gini', maxDepth=20, maxBins=20)\n",
    "    \n",
    "predictions = model.predict(<FILL IN>.map(lambda x: x.features))\n",
    "labelsAndPredictions_DecisionTree = parsedTestData2.map(lambda lp: lp.label).zip(predictions)\n",
    "# Calculate accuracy our test prediction for model DecisionTree\n",
    "test_acc_DecisionTree = labelsAndPredictions_DecisionTree.filter(lambda (v, p): v == p).count() / float(parsedTestData2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(test_acc_DecisionTree>=0.41,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong DecisionTree model prediction', \n",
    "                        'Test 23 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Train RandomForest model and predict it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "model = <FILL IN>(<FILL IN>, numClasses=6, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "predictions = <FILL IN>\n",
    "# Calculate accuracy our test prediction for model DecisionTree\n",
    "labelsAndPredictions_RandomForest = parsedTestData2.map(lambda lp: lp.label).zip(predictions)\n",
    "test_acc_RandomForest = labelsAndPredictions_RandomForest.filter(lambda (v, p): v == p).count() / float(parsedTestData2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(test_acc_RandomForest>=0.4,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong RandomForest model prediction', \n",
    "                        'Test 24 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy = ' + str(test_acc_DecisionTree))\n",
    "print('Test Accuracy = ' + str(test_acc_RandomForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERCISE 6\n",
    "#### **MLlib's [LogisticRegressionWithSGD](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=logisticregressionwithsgd#pyspark.mllib.classification.LogisticRegressionWithSGD)** and **[SVMWithSGD](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=svmwithsgd#pyspark.mllib.classification.SVMWithSGD)**\n",
    "#### Create **period_binary()** function to get two labels for each millennium (1.0 for current millennium, and 0.0 - for previous). Get **parsedData3**, **parsedTrainData3**, **parsedTestData3** with this function and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def period_binary(in_line):\n",
    "    <FILL IN>\n",
    "    return in_line\n",
    "\n",
    "parsedData3 = <FILL IN>\n",
    "(parsedTrainData3, parsedTestData3) = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(parsedData3.map(lambda lp: lp.label).sum(),'2f96c1c8410ef66dfd763f0810550308d2879777', \n",
    "                        'Wrong period labels in Data', 'Test 25 success')\n",
    "Test.assertEqualsHashed((parsedTrainData3.map(lambda lp: lp.label).sum()+parsedTestData3.map(lambda lp: lp.label).sum()),\n",
    "                        '2f96c1c8410ef66dfd763f0810550308d2879777', 'Incorect splitting', 'Test 26 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traine and predict LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model = <FILL IN>.train(<FILL IN>)\n",
    "\n",
    "predictions = model.<FILL IN>\n",
    "\n",
    "labelsAndPreds_LogisticRegression = parsedTestData3.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = labelsAndPreds_LogisticRegression.filter(lambda (v, p): v == p).count() / float(parsedTestData3.count())\n",
    "Test.assertEqualsHashed(a>=0.82,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong LogisticRegression model prediction', \n",
    "                        'Test 27 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate test error (opposite to training accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "trainErr_LogisticRegression = <FILL IN>\n",
    "print(\"Test Error = \" + str(trainErr_LogisticRegression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traine and predict SVMWithSGD. Then calculate test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model = <FILL IN>\n",
    "\n",
    "labelsAndPreds_SVMWithSGD = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = labelsAndPreds_SVMWithSGD.filter(lambda (v, p): v == p).count() / float(parsedTestData3.count())\n",
    "Test.assertEqualsHashed(a>=0.82,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong SVMWithSGD model prediction', \n",
    "                        'Test 28 success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainErr_SVMWithSGD = <FILL IN>\n",
    "# Comparison test error LogisticRegressionWithSGD and SVMWithSGD\n",
    "print(\"Test Error = \" + str(trainErr_LogisticRegression))\n",
    "print(\"Test Error = \" + str(trainErr_SVMWithSGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test.assertEqualsHashed(trainErr_LogisticRegression<=0.18,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong LogisticRegression err calculation', \n",
    "                        'Test 29 success')\n",
    "Test.assertEqualsHashed(trainErr_SVMWithSGD<=0.18,'88b33e4e12f75ac8bf792aebde41f1a090f3a612', 'Wrong SVMWithSGD err calculation', \n",
    "                        'Test 30 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Presented by <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"http://datascience-school.com\">datascience-school.com</a></h3></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

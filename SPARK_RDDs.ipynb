{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Spark Tutorial: Learning Apache Spark**\n",
    "#### [Apache Spark](http://spark.apache.org/) is a cluster computing platform designed to be fast and general purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every Spark application consists of a driver program that launches paralel operations on a cluster. Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n",
    "#### *Spark Context*\n",
    "#### We can execute python code using this IPython notebook. But, sisnce no Spark functionality is actually being used, no tasks are launched on the executors. In order to use Spark and its API we will need to use a `SparkContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the type of the Spark Context sc\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformations and actions with RDDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since RDD is immutable, Users create RDDs in two ways: by loading an external dataset, or by running Transformation on a pre-existing RDD. \n",
    "####There are second type of operations called Actions. They compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**Lets create our first RDD**   \n",
    "####First, we generate dummy data by creating a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = xrange(1, 4561,2)\n",
    "\n",
    "\n",
    "# We can check the size of the list and print out its first element\n",
    "\n",
    "print len(data)\n",
    "print data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### To create the RDD, we use `sc.parallelize()`, which tells Spark to create a new set of input data based on data that is passed in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallelize data using 8 partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "firstRDD = sc.parallelize(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformations**\n",
    "####One of the most common transformation is Map.  Essentially, it applies a function to each element of the dataset, and outputs resulting dataset of the same length. \n",
    "#### Now we will use `map()` to increase per unit each value in the  firstRDD we just created. \n",
    "#### It is important to remember, that Spark passes functions. So, we have to create a function or use lambda (unnamed) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets use lambda function. It should be: lambda x: x+1\n",
    "plusOneRDD=firstRDD.map(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Actions  **\n",
    "#### To see the resulting list we can usethe function collect().\n",
    "#### The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the RDD returned by the action.  In our example, this means that tasks will now be launched to perform the `parallelize`, `map`, and `collect` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 38,\n",
       " 40,\n",
       " 42,\n",
       " 44,\n",
       " 46,\n",
       " 48,\n",
       " 50,\n",
       " 52,\n",
       " 54,\n",
       " 56,\n",
       " 58,\n",
       " 60,\n",
       " 62,\n",
       " 64,\n",
       " 66,\n",
       " 68,\n",
       " 70,\n",
       " 72,\n",
       " 74,\n",
       " 76,\n",
       " 78,\n",
       " 80,\n",
       " 82,\n",
       " 84,\n",
       " 86,\n",
       " 88,\n",
       " 90,\n",
       " 92,\n",
       " 94,\n",
       " 96,\n",
       " 98,\n",
       " 100,\n",
       " 102,\n",
       " 104,\n",
       " 106,\n",
       " 108,\n",
       " 110,\n",
       " 112,\n",
       " 114,\n",
       " 116,\n",
       " 118,\n",
       " 120,\n",
       " 122,\n",
       " 124,\n",
       " 126,\n",
       " 128,\n",
       " 130,\n",
       " 132,\n",
       " 134,\n",
       " 136,\n",
       " 138,\n",
       " 140,\n",
       " 142,\n",
       " 144,\n",
       " 146,\n",
       " 148,\n",
       " 150,\n",
       " 152,\n",
       " 154,\n",
       " 156,\n",
       " 158,\n",
       " 160,\n",
       " 162,\n",
       " 164,\n",
       " 166,\n",
       " 168,\n",
       " 170,\n",
       " 172,\n",
       " 174,\n",
       " 176,\n",
       " 178,\n",
       " 180,\n",
       " 182,\n",
       " 184,\n",
       " 186,\n",
       " 188,\n",
       " 190,\n",
       " 192,\n",
       " 194,\n",
       " 196,\n",
       " 198,\n",
       " 200,\n",
       " 202,\n",
       " 204,\n",
       " 206,\n",
       " 208,\n",
       " 210,\n",
       " 212,\n",
       " 214,\n",
       " 216,\n",
       " 218,\n",
       " 220,\n",
       " 222,\n",
       " 224,\n",
       " 226,\n",
       " 228,\n",
       " 230,\n",
       " 232,\n",
       " 234,\n",
       " 236,\n",
       " 238,\n",
       " 240,\n",
       " 242,\n",
       " 244,\n",
       " 246,\n",
       " 248,\n",
       " 250,\n",
       " 252,\n",
       " 254,\n",
       " 256,\n",
       " 258,\n",
       " 260,\n",
       " 262,\n",
       " 264,\n",
       " 266,\n",
       " 268,\n",
       " 270,\n",
       " 272,\n",
       " 274,\n",
       " 276,\n",
       " 278,\n",
       " 280,\n",
       " 282,\n",
       " 284,\n",
       " 286,\n",
       " 288,\n",
       " 290,\n",
       " 292,\n",
       " 294,\n",
       " 296,\n",
       " 298,\n",
       " 300,\n",
       " 302,\n",
       " 304,\n",
       " 306,\n",
       " 308,\n",
       " 310,\n",
       " 312,\n",
       " 314,\n",
       " 316,\n",
       " 318,\n",
       " 320,\n",
       " 322,\n",
       " 324,\n",
       " 326,\n",
       " 328,\n",
       " 330,\n",
       " 332,\n",
       " 334,\n",
       " 336,\n",
       " 338,\n",
       " 340,\n",
       " 342,\n",
       " 344,\n",
       " 346,\n",
       " 348,\n",
       " 350,\n",
       " 352,\n",
       " 354,\n",
       " 356,\n",
       " 358,\n",
       " 360,\n",
       " 362,\n",
       " 364,\n",
       " 366,\n",
       " 368,\n",
       " 370,\n",
       " 372,\n",
       " 374,\n",
       " 376,\n",
       " 378,\n",
       " 380,\n",
       " 382,\n",
       " 384,\n",
       " 386,\n",
       " 388,\n",
       " 390,\n",
       " 392,\n",
       " 394,\n",
       " 396,\n",
       " 398,\n",
       " 400,\n",
       " 402,\n",
       " 404,\n",
       " 406,\n",
       " 408,\n",
       " 410,\n",
       " 412,\n",
       " 414,\n",
       " 416,\n",
       " 418,\n",
       " 420,\n",
       " 422,\n",
       " 424,\n",
       " 426,\n",
       " 428,\n",
       " 430,\n",
       " 432,\n",
       " 434,\n",
       " 436,\n",
       " 438,\n",
       " 440,\n",
       " 442,\n",
       " 444,\n",
       " 446,\n",
       " 448,\n",
       " 450,\n",
       " 452,\n",
       " 454,\n",
       " 456,\n",
       " 458,\n",
       " 460,\n",
       " 462,\n",
       " 464,\n",
       " 466,\n",
       " 468,\n",
       " 470,\n",
       " 472,\n",
       " 474,\n",
       " 476,\n",
       " 478,\n",
       " 480,\n",
       " 482,\n",
       " 484,\n",
       " 486,\n",
       " 488,\n",
       " 490,\n",
       " 492,\n",
       " 494,\n",
       " 496,\n",
       " 498,\n",
       " 500,\n",
       " 502,\n",
       " 504,\n",
       " 506,\n",
       " 508,\n",
       " 510,\n",
       " 512,\n",
       " 514,\n",
       " 516,\n",
       " 518,\n",
       " 520,\n",
       " 522,\n",
       " 524,\n",
       " 526,\n",
       " 528,\n",
       " 530,\n",
       " 532,\n",
       " 534,\n",
       " 536,\n",
       " 538,\n",
       " 540,\n",
       " 542,\n",
       " 544,\n",
       " 546,\n",
       " 548,\n",
       " 550,\n",
       " 552,\n",
       " 554,\n",
       " 556,\n",
       " 558,\n",
       " 560,\n",
       " 562,\n",
       " 564,\n",
       " 566,\n",
       " 568,\n",
       " 570,\n",
       " 572,\n",
       " 574,\n",
       " 576,\n",
       " 578,\n",
       " 580,\n",
       " 582,\n",
       " 584,\n",
       " 586,\n",
       " 588,\n",
       " 590,\n",
       " 592,\n",
       " 594,\n",
       " 596,\n",
       " 598,\n",
       " 600,\n",
       " 602,\n",
       " 604,\n",
       " 606,\n",
       " 608,\n",
       " 610,\n",
       " 612,\n",
       " 614,\n",
       " 616,\n",
       " 618,\n",
       " 620,\n",
       " 622,\n",
       " 624,\n",
       " 626,\n",
       " 628,\n",
       " 630,\n",
       " 632,\n",
       " 634,\n",
       " 636,\n",
       " 638,\n",
       " 640,\n",
       " 642,\n",
       " 644,\n",
       " 646,\n",
       " 648,\n",
       " 650,\n",
       " 652,\n",
       " 654,\n",
       " 656,\n",
       " 658,\n",
       " 660,\n",
       " 662,\n",
       " 664,\n",
       " 666,\n",
       " 668,\n",
       " 670,\n",
       " 672,\n",
       " 674,\n",
       " 676,\n",
       " 678,\n",
       " 680,\n",
       " 682,\n",
       " 684,\n",
       " 686,\n",
       " 688,\n",
       " 690,\n",
       " 692,\n",
       " 694,\n",
       " 696,\n",
       " 698,\n",
       " 700,\n",
       " 702,\n",
       " 704,\n",
       " 706,\n",
       " 708,\n",
       " 710,\n",
       " 712,\n",
       " 714,\n",
       " 716,\n",
       " 718,\n",
       " 720,\n",
       " 722,\n",
       " 724,\n",
       " 726,\n",
       " 728,\n",
       " 730,\n",
       " 732,\n",
       " 734,\n",
       " 736,\n",
       " 738,\n",
       " 740,\n",
       " 742,\n",
       " 744,\n",
       " 746,\n",
       " 748,\n",
       " 750,\n",
       " 752,\n",
       " 754,\n",
       " 756,\n",
       " 758,\n",
       " 760,\n",
       " 762,\n",
       " 764,\n",
       " 766,\n",
       " 768,\n",
       " 770,\n",
       " 772,\n",
       " 774,\n",
       " 776,\n",
       " 778,\n",
       " 780,\n",
       " 782,\n",
       " 784,\n",
       " 786,\n",
       " 788,\n",
       " 790,\n",
       " 792,\n",
       " 794,\n",
       " 796,\n",
       " 798,\n",
       " 800,\n",
       " 802,\n",
       " 804,\n",
       " 806,\n",
       " 808,\n",
       " 810,\n",
       " 812,\n",
       " 814,\n",
       " 816,\n",
       " 818,\n",
       " 820,\n",
       " 822,\n",
       " 824,\n",
       " 826,\n",
       " 828,\n",
       " 830,\n",
       " 832,\n",
       " 834,\n",
       " 836,\n",
       " 838,\n",
       " 840,\n",
       " 842,\n",
       " 844,\n",
       " 846,\n",
       " 848,\n",
       " 850,\n",
       " 852,\n",
       " 854,\n",
       " 856,\n",
       " 858,\n",
       " 860,\n",
       " 862,\n",
       " 864,\n",
       " 866,\n",
       " 868,\n",
       " 870,\n",
       " 872,\n",
       " 874,\n",
       " 876,\n",
       " 878,\n",
       " 880,\n",
       " 882,\n",
       " 884,\n",
       " 886,\n",
       " 888,\n",
       " 890,\n",
       " 892,\n",
       " 894,\n",
       " 896,\n",
       " 898,\n",
       " 900,\n",
       " 902,\n",
       " 904,\n",
       " 906,\n",
       " 908,\n",
       " 910,\n",
       " 912,\n",
       " 914,\n",
       " 916,\n",
       " 918,\n",
       " 920,\n",
       " 922,\n",
       " 924,\n",
       " 926,\n",
       " 928,\n",
       " 930,\n",
       " 932,\n",
       " 934,\n",
       " 936,\n",
       " 938,\n",
       " 940,\n",
       " 942,\n",
       " 944,\n",
       " 946,\n",
       " 948,\n",
       " 950,\n",
       " 952,\n",
       " 954,\n",
       " 956,\n",
       " 958,\n",
       " 960,\n",
       " 962,\n",
       " 964,\n",
       " 966,\n",
       " 968,\n",
       " 970,\n",
       " 972,\n",
       " 974,\n",
       " 976,\n",
       " 978,\n",
       " 980,\n",
       " 982,\n",
       " 984,\n",
       " 986,\n",
       " 988,\n",
       " 990,\n",
       " 992,\n",
       " 994,\n",
       " 996,\n",
       " 998,\n",
       " 1000,\n",
       " 1002,\n",
       " 1004,\n",
       " 1006,\n",
       " 1008,\n",
       " 1010,\n",
       " 1012,\n",
       " 1014,\n",
       " 1016,\n",
       " 1018,\n",
       " 1020,\n",
       " 1022,\n",
       " 1024,\n",
       " 1026,\n",
       " 1028,\n",
       " 1030,\n",
       " 1032,\n",
       " 1034,\n",
       " 1036,\n",
       " 1038,\n",
       " 1040,\n",
       " 1042,\n",
       " 1044,\n",
       " 1046,\n",
       " 1048,\n",
       " 1050,\n",
       " 1052,\n",
       " 1054,\n",
       " 1056,\n",
       " 1058,\n",
       " 1060,\n",
       " 1062,\n",
       " 1064,\n",
       " 1066,\n",
       " 1068,\n",
       " 1070,\n",
       " 1072,\n",
       " 1074,\n",
       " 1076,\n",
       " 1078,\n",
       " 1080,\n",
       " 1082,\n",
       " 1084,\n",
       " 1086,\n",
       " 1088,\n",
       " 1090,\n",
       " 1092,\n",
       " 1094,\n",
       " 1096,\n",
       " 1098,\n",
       " 1100,\n",
       " 1102,\n",
       " 1104,\n",
       " 1106,\n",
       " 1108,\n",
       " 1110,\n",
       " 1112,\n",
       " 1114,\n",
       " 1116,\n",
       " 1118,\n",
       " 1120,\n",
       " 1122,\n",
       " 1124,\n",
       " 1126,\n",
       " 1128,\n",
       " 1130,\n",
       " 1132,\n",
       " 1134,\n",
       " 1136,\n",
       " 1138,\n",
       " 1140,\n",
       " 1142,\n",
       " 1144,\n",
       " 1146,\n",
       " 1148,\n",
       " 1150,\n",
       " 1152,\n",
       " 1154,\n",
       " 1156,\n",
       " 1158,\n",
       " 1160,\n",
       " 1162,\n",
       " 1164,\n",
       " 1166,\n",
       " 1168,\n",
       " 1170,\n",
       " 1172,\n",
       " 1174,\n",
       " 1176,\n",
       " 1178,\n",
       " 1180,\n",
       " 1182,\n",
       " 1184,\n",
       " 1186,\n",
       " 1188,\n",
       " 1190,\n",
       " 1192,\n",
       " 1194,\n",
       " 1196,\n",
       " 1198,\n",
       " 1200,\n",
       " 1202,\n",
       " 1204,\n",
       " 1206,\n",
       " 1208,\n",
       " 1210,\n",
       " 1212,\n",
       " 1214,\n",
       " 1216,\n",
       " 1218,\n",
       " 1220,\n",
       " 1222,\n",
       " 1224,\n",
       " 1226,\n",
       " 1228,\n",
       " 1230,\n",
       " 1232,\n",
       " 1234,\n",
       " 1236,\n",
       " 1238,\n",
       " 1240,\n",
       " 1242,\n",
       " 1244,\n",
       " 1246,\n",
       " 1248,\n",
       " 1250,\n",
       " 1252,\n",
       " 1254,\n",
       " 1256,\n",
       " 1258,\n",
       " 1260,\n",
       " 1262,\n",
       " 1264,\n",
       " 1266,\n",
       " 1268,\n",
       " 1270,\n",
       " 1272,\n",
       " 1274,\n",
       " 1276,\n",
       " 1278,\n",
       " 1280,\n",
       " 1282,\n",
       " 1284,\n",
       " 1286,\n",
       " 1288,\n",
       " 1290,\n",
       " 1292,\n",
       " 1294,\n",
       " 1296,\n",
       " 1298,\n",
       " 1300,\n",
       " 1302,\n",
       " 1304,\n",
       " 1306,\n",
       " 1308,\n",
       " 1310,\n",
       " 1312,\n",
       " 1314,\n",
       " 1316,\n",
       " 1318,\n",
       " 1320,\n",
       " 1322,\n",
       " 1324,\n",
       " 1326,\n",
       " 1328,\n",
       " 1330,\n",
       " 1332,\n",
       " 1334,\n",
       " 1336,\n",
       " 1338,\n",
       " 1340,\n",
       " 1342,\n",
       " 1344,\n",
       " 1346,\n",
       " 1348,\n",
       " 1350,\n",
       " 1352,\n",
       " 1354,\n",
       " 1356,\n",
       " 1358,\n",
       " 1360,\n",
       " 1362,\n",
       " 1364,\n",
       " 1366,\n",
       " 1368,\n",
       " 1370,\n",
       " 1372,\n",
       " 1374,\n",
       " 1376,\n",
       " 1378,\n",
       " 1380,\n",
       " 1382,\n",
       " 1384,\n",
       " 1386,\n",
       " 1388,\n",
       " 1390,\n",
       " 1392,\n",
       " 1394,\n",
       " 1396,\n",
       " 1398,\n",
       " 1400,\n",
       " 1402,\n",
       " 1404,\n",
       " 1406,\n",
       " 1408,\n",
       " 1410,\n",
       " 1412,\n",
       " 1414,\n",
       " 1416,\n",
       " 1418,\n",
       " 1420,\n",
       " 1422,\n",
       " 1424,\n",
       " 1426,\n",
       " 1428,\n",
       " 1430,\n",
       " 1432,\n",
       " 1434,\n",
       " 1436,\n",
       " 1438,\n",
       " 1440,\n",
       " 1442,\n",
       " 1444,\n",
       " 1446,\n",
       " 1448,\n",
       " 1450,\n",
       " 1452,\n",
       " 1454,\n",
       " 1456,\n",
       " 1458,\n",
       " 1460,\n",
       " 1462,\n",
       " 1464,\n",
       " 1466,\n",
       " 1468,\n",
       " 1470,\n",
       " 1472,\n",
       " 1474,\n",
       " 1476,\n",
       " 1478,\n",
       " 1480,\n",
       " 1482,\n",
       " 1484,\n",
       " 1486,\n",
       " 1488,\n",
       " 1490,\n",
       " 1492,\n",
       " 1494,\n",
       " 1496,\n",
       " 1498,\n",
       " 1500,\n",
       " 1502,\n",
       " 1504,\n",
       " 1506,\n",
       " 1508,\n",
       " 1510,\n",
       " 1512,\n",
       " 1514,\n",
       " 1516,\n",
       " 1518,\n",
       " 1520,\n",
       " 1522,\n",
       " 1524,\n",
       " 1526,\n",
       " 1528,\n",
       " 1530,\n",
       " 1532,\n",
       " 1534,\n",
       " 1536,\n",
       " 1538,\n",
       " 1540,\n",
       " 1542,\n",
       " 1544,\n",
       " 1546,\n",
       " 1548,\n",
       " 1550,\n",
       " 1552,\n",
       " 1554,\n",
       " 1556,\n",
       " 1558,\n",
       " 1560,\n",
       " 1562,\n",
       " 1564,\n",
       " 1566,\n",
       " 1568,\n",
       " 1570,\n",
       " 1572,\n",
       " 1574,\n",
       " 1576,\n",
       " 1578,\n",
       " 1580,\n",
       " 1582,\n",
       " 1584,\n",
       " 1586,\n",
       " 1588,\n",
       " 1590,\n",
       " 1592,\n",
       " 1594,\n",
       " 1596,\n",
       " 1598,\n",
       " 1600,\n",
       " 1602,\n",
       " 1604,\n",
       " 1606,\n",
       " 1608,\n",
       " 1610,\n",
       " 1612,\n",
       " 1614,\n",
       " 1616,\n",
       " 1618,\n",
       " 1620,\n",
       " 1622,\n",
       " 1624,\n",
       " 1626,\n",
       " 1628,\n",
       " 1630,\n",
       " 1632,\n",
       " 1634,\n",
       " 1636,\n",
       " 1638,\n",
       " 1640,\n",
       " 1642,\n",
       " 1644,\n",
       " 1646,\n",
       " 1648,\n",
       " 1650,\n",
       " 1652,\n",
       " 1654,\n",
       " 1656,\n",
       " 1658,\n",
       " 1660,\n",
       " 1662,\n",
       " 1664,\n",
       " 1666,\n",
       " 1668,\n",
       " 1670,\n",
       " 1672,\n",
       " 1674,\n",
       " 1676,\n",
       " 1678,\n",
       " 1680,\n",
       " 1682,\n",
       " 1684,\n",
       " 1686,\n",
       " 1688,\n",
       " 1690,\n",
       " 1692,\n",
       " 1694,\n",
       " 1696,\n",
       " 1698,\n",
       " 1700,\n",
       " 1702,\n",
       " 1704,\n",
       " 1706,\n",
       " 1708,\n",
       " 1710,\n",
       " 1712,\n",
       " 1714,\n",
       " 1716,\n",
       " 1718,\n",
       " 1720,\n",
       " 1722,\n",
       " 1724,\n",
       " 1726,\n",
       " 1728,\n",
       " 1730,\n",
       " 1732,\n",
       " 1734,\n",
       " 1736,\n",
       " 1738,\n",
       " 1740,\n",
       " 1742,\n",
       " 1744,\n",
       " 1746,\n",
       " 1748,\n",
       " 1750,\n",
       " 1752,\n",
       " 1754,\n",
       " 1756,\n",
       " 1758,\n",
       " 1760,\n",
       " 1762,\n",
       " 1764,\n",
       " 1766,\n",
       " 1768,\n",
       " 1770,\n",
       " 1772,\n",
       " 1774,\n",
       " 1776,\n",
       " 1778,\n",
       " 1780,\n",
       " 1782,\n",
       " 1784,\n",
       " 1786,\n",
       " 1788,\n",
       " 1790,\n",
       " 1792,\n",
       " 1794,\n",
       " 1796,\n",
       " 1798,\n",
       " 1800,\n",
       " 1802,\n",
       " 1804,\n",
       " 1806,\n",
       " 1808,\n",
       " 1810,\n",
       " 1812,\n",
       " 1814,\n",
       " 1816,\n",
       " 1818,\n",
       " 1820,\n",
       " 1822,\n",
       " 1824,\n",
       " 1826,\n",
       " 1828,\n",
       " 1830,\n",
       " 1832,\n",
       " 1834,\n",
       " 1836,\n",
       " 1838,\n",
       " 1840,\n",
       " 1842,\n",
       " 1844,\n",
       " 1846,\n",
       " 1848,\n",
       " 1850,\n",
       " 1852,\n",
       " 1854,\n",
       " 1856,\n",
       " 1858,\n",
       " 1860,\n",
       " 1862,\n",
       " 1864,\n",
       " 1866,\n",
       " 1868,\n",
       " 1870,\n",
       " 1872,\n",
       " 1874,\n",
       " 1876,\n",
       " 1878,\n",
       " 1880,\n",
       " 1882,\n",
       " 1884,\n",
       " 1886,\n",
       " 1888,\n",
       " 1890,\n",
       " 1892,\n",
       " 1894,\n",
       " 1896,\n",
       " 1898,\n",
       " 1900,\n",
       " 1902,\n",
       " 1904,\n",
       " 1906,\n",
       " 1908,\n",
       " 1910,\n",
       " 1912,\n",
       " 1914,\n",
       " 1916,\n",
       " 1918,\n",
       " 1920,\n",
       " 1922,\n",
       " 1924,\n",
       " 1926,\n",
       " 1928,\n",
       " 1930,\n",
       " 1932,\n",
       " 1934,\n",
       " 1936,\n",
       " 1938,\n",
       " 1940,\n",
       " 1942,\n",
       " 1944,\n",
       " 1946,\n",
       " 1948,\n",
       " 1950,\n",
       " 1952,\n",
       " 1954,\n",
       " 1956,\n",
       " 1958,\n",
       " 1960,\n",
       " 1962,\n",
       " 1964,\n",
       " 1966,\n",
       " 1968,\n",
       " 1970,\n",
       " 1972,\n",
       " 1974,\n",
       " 1976,\n",
       " 1978,\n",
       " 1980,\n",
       " 1982,\n",
       " 1984,\n",
       " 1986,\n",
       " 1988,\n",
       " 1990,\n",
       " 1992,\n",
       " 1994,\n",
       " 1996,\n",
       " 1998,\n",
       " 2000,\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's collect the data\n",
    "plusOneRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2930, 1526, 3492, 2654, 1628, 4300, 2794, 1432, 4228, 1426]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plusOneRDD.takeSample(True,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another usefull action is count(). It counts the number of elements in an RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n"
     ]
    }
   ],
   "source": [
    "print plusOneRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# by using map() and lambda function add 5 to each element of firstRDD\n",
    "plusfiveRDD = firstRDD.<FILL IN>\n",
    "\n",
    "print plusfiveRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "import sys\n",
    "sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "\n",
    "import hashlib\n",
    "hashed = lambda x: hashlib.sha1(str(x)).hexdigest()\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "f = plusfiveRDD.first()\n",
    "\n",
    "\n",
    "Test.assertEqualsHashed(f, 'c1dfd96eea8cc2b62785275bca38ac261256e278', 'Test 1 success')\n",
    "Test.assertEqualsHashed(plusfiveRDD.take(10)[-1], '4d134bc072212ace2df385dae143139da74ec0ef', 'Test 2 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's filter our plusOneRDD. \n",
    "#### We'll create a new RDD that only contains the values less than 100 by using the `filter(f)` data-parallel operation. This method is a transformation operation that creates a new RDD from the input RDD by applying filter function `f` to each item in the parent RDD and only passing those elements where the filter function returns `True`. Elements that do not return `True` will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to filter a single value\n",
    "def flt(value):\n",
    "    if (value < 100):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "filteredRDD = plusOneRDD.filter(flt)\n",
    "\n",
    "# Since filter is a transformation, we have to use action to preform calculations \n",
    "# and get resulting data\n",
    "print filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we allow you filter your plusfiveRDD with numbers divided by 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# define the function that check or value is divide by 3 or not\n",
    "def devided_by_3_number(value):\n",
    "    <FILL IN>\n",
    "    return <FILL IN>\n",
    "\n",
    "# filter your RDD\n",
    "filteredNewRDD = plusfiveRDD.<FILL IN>\n",
    "\n",
    "# print the list of number\n",
    "print <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "from test_helper import Test\n",
    "\n",
    "f_10 = filteredNewRDD.take(10)\n",
    "t_1 = filteredNewRDD.top(1)[0]\n",
    "\n",
    "\n",
    "Test.assertEqualsHashed(f_10, '5199c0bbcd0c8779c4cf6cb9cec2b9619ce161ca', 'Test 3 success')\n",
    "Test.assertEqualsHashed(t_1, 'f2454d82ad9bd30810f80d8f76ce3ea3e50c9ce5', 'Test 4 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Data displaying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are few other frequently used actions:[first()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first), [take()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take), [top()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top), [takeOrdered()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered), and [reduce().](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)\n",
    "#### In order to get rough understanding of the data through visual inspection    `first()`, `take()`, `top()`, and `takeOrdered()` actions are used. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the RDD is *partitioned*.\n",
    "#### The `take(n)` action to returns the first n elements of the RDD. \n",
    "####The `first()` action returns the first element of an RDD, and is equivalent to `take(1)`.\n",
    "#### The `takeOrdered(n)` action returns the first n elements of the RDD, using either their natural order or a custom comparator. \n",
    "####The `top(n)` action is similar to `takeOrdered(n)` except that it returns the list in *descending order.*\n",
    "#### The `reduce()` action reduces the elements of a RDD to a single value by applying a function that takes two parameters and returns a single value.  The function should be commutative and associative, as `reduce()` is applied at the partition level and then again to aggregate results from partitions.  If these rules don't hold, the results from `reduce()` will be inconsistent.  Reducing locally at partitions makes `reduce()` very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the first element\n",
    "print filteredRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first 4\n",
    "print filteredRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the three smallest elements\n",
    "print filteredRDD.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the five largest elements\n",
    "print filteredRDD.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass a lambda function to takeOrdered to reverse the order\n",
    "filteredRDD.takeOrdered(4, lambda s: -s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting Python's add function\n",
    "from operator import add\n",
    "# Sum the RDD using reduce\n",
    "print filteredRDD.reduce(add)\n",
    "# Sum using reduce with a lambda function\n",
    "print filteredRDD.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# filter your filteredNewRDD with number which are less than 50\n",
    "filteredNewRDD_part = filteredNewRDD.<FILL IN>\n",
    "\n",
    "# Print top 5 the greatest numbers in your filteredNewRDD_part\n",
    "filteredNewRDD_5 = filteredNewRDD_part.<FILL IN>\n",
    "\n",
    "\n",
    "# Multiply all numbers from  filteredNewRDD_part and print result\n",
    "Multi_filteredRDD = filteredNewRDD_part.<FILL IN>\n",
    "\n",
    "\n",
    "print Multi_filteredRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "Test.assertEqualsHashed(filteredNewRDD_5, '8ff5b2b55bc143e048bf78182ec3194f1e49e0c7', 'Test 5 success')\n",
    "Test.assertEqualsHashed(Multi_filteredRDD, 'cb78c76bd7a7cfe8feb17ad7a4e1166dcfbbb7d3', 'Test 6 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `countByValue()` action returns the count of each unique value in the RDD as a dictionary that maps values to counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newRDD = sc.parallelize([\"d\", \"s\", \"d\", \"d\", \"r\", \"ee\", \"rr\", \"r\", \"r\"])\n",
    "print newRDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Define the text variable \"I have a cat. The cat is very nice. I love my cat very much.\"\n",
    "\n",
    "text = <FILL IN>\n",
    "\n",
    "\n",
    "# Create RDD with all letters and symbols from the text\n",
    "# Before remove all spaces from text and move all latters to lowercase\n",
    "# here you can use the library re (read about it here https://docs.python.org/2/library/re.html) or the function 'replace()'\n",
    "\n",
    "textRDD = <FILL IN>\n",
    "\n",
    "# Count the number of each symbol \n",
    "\n",
    "n = textRDD.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "Test.assertEqualsHashed(len(n), '1574bddb75c78a6fd2251d61e2993b5146201319', 'Test 7 success')\n",
    "Test.assertEqualsHashed(textRDD.take(20)[10], '8efd86fb78a56a5145ed7739dcb00c78581c5375', 'Test 8 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** More RDD transformations **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   [flatMap()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) transformation is similar to `map()`, except that with `flatMap()` each input item can be mapped to zero or more output elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating new RDD\n",
    "data = ['foo', 'bar', '1', '2']\n",
    "dataRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# From each peice of data create pair that consists from original data and original data + letter 's'. \n",
    "# Use map\n",
    "pairsRDD = dataRDD.<FILL IN>\n",
    "\n",
    "# In this section you should create the pairs ('foo', 'foods') and than use flatMap to create a rdd like ['foo', 'foos', ...]\n",
    "pairsRDDfmap = dataRDD.<FILL IN>\n",
    "\n",
    "# View the results with collecting the data\n",
    "print pairsRDD.<FILL IN>\n",
    "print pairsRDDfmap.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "Test.assertEqualsHashed(pairsRDD.count(), '1b6453892473a467d07372d45eb05abc2031647a', 'Test 9 success')\n",
    "Test.assertEqualsHashed(pairsRDDfmap.count(), 'fe5dbbcea5ce7e2988b8c69bcfdfde8904aabc1f', 'Test 10 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The approach using map transformation is often used to create key-value pairs.\n",
    "\n",
    "\n",
    "#### Let's investigate transformations that are used with key-value pairs: [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) and [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey).\n",
    "\n",
    "#### The `reduceByKey()` transformation gathers together pairs that have the same key and applies a function to two associated values at a time. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions.\n",
    "#### While both the `groupByKey()` and `reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `reduceByKey()` transformation works much better for large distributed datasets. \n",
    "\n",
    "#### Here are more transformations to prefer over `groupByKey()`:\n",
    "  + #### [combineByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.combineByKey) \n",
    "####can be used when you are combining elements but your return type differs from your input value type.\n",
    "  + #### [foldByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.foldByKey) \n",
    "#### merges the values for each key using an associative function and a neutral \"zero value\".\n",
    "#### Now let's go through a simple `groupByKey()` and `reduceByKey()` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values=[\"apple\", \"banana\", \"apple\", \"papaya\", \"mango\", \"prune\", \"mango\"]\n",
    "valuesRDD = sc.parallelize(values,4)\n",
    "keyvalueRDD=valuesRDD.map(lambda x: (x,1))\n",
    "\n",
    "# Different ways to sum by key\n",
    "print keyvalueRDD.groupByKey().map(lambda (k, v): (k, sum(v))).collect()\n",
    "# Using mapValues, which is recommended when they key doesn't change\n",
    "print keyvalueRDD.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "# reduceByKey is more efficient / scalable\n",
    "print keyvalueRDD.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we have need to get a list of distinct elements we can use 'distinct()' transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print valuesRDD.collect()\n",
    "print valuesRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_list = [1, 3, 4, 1, 4, 7, 12, 3, 4, 2, 2, 6, 2, 1, 9]\n",
    "\n",
    "# Create RDD with numbers\n",
    "valuesRDD_num = <FILL IN>\n",
    "\n",
    "# Create key\\value pair of RDD with each value of number_list and 1\n",
    "keyvalueRDD_num = valuesRDD_num.<FILL IN>\n",
    "\n",
    "# Use groupByKey()  or reduceByKey()  to count the number of duplicates of each element\n",
    "# Print list from key value RDD for numbers 1, 2, 3\n",
    "\n",
    "keyvalueRDD_num_ordered = keyvalueRDD_num.<FILL IN>\n",
    "keyvalueRDD_num_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "Test.assertEqualsHashed(keyvalueRDD_num.take(3)[0], 'd17c7c5e7c759de38db5cb6577a9a60e8ef9456a', 'Test 11 success')\n",
    "Test.assertEqualsHashed(keyvalueRDD_num_ordered, 'd0a4b212ce0266e5c798105a7312ef88425b6c62', 'Test 12 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Caching RDDs **\n",
    "#### If you need to use the same RDD more than once it can be usefull to  cache it using function cache(). However, if you cache too many RDDs and Spark runs out of memory, it will delete the least recently used (LRU) RDD first. Again, the RDD will be automatically recreated when accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name the RDD\n",
    "filteredRDD.setName('My Filtered RDD')\n",
    "# Cache the RDD\n",
    "filteredRDD.cache()\n",
    "# Is it cached\n",
    "print filteredRDD.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Let's have some practice: Simple inventory management system **\n",
    "#### \n",
    "RDD1- 1 dep\n",
    "RDD2- 2 dep\n",
    "RDDall\n",
    "\n",
    "Diff 1/2\n",
    "Add objects\n",
    "Catalog all\n",
    "\n",
    "\n",
    "\n",
    "create 2 RDD ,    compare join key-value pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theese are the lists of inventories of the 2 warehouses\n",
    "inventory1=['Hammer', 'nail', 'Nail', 'screwdriver', 'Backpack', 'Bolt D9', 'Nut D9','Bolt D9', 'Nut D9','Bolt D9', 'nut D12','Bolt D12', 'nut D9','Bolt D9', 'Nut D12']\n",
    "inventory2=['Bolt D8', 'nut D8','Screwdriver', 'Backpack', 'Bolt D9','screwdriver', 'backpack', 'Bolt D9', 'First Aid Kit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create an RDD for each warehouse. Be aware that some goods are written with uppercase letters. You have to use lower() function from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD1=sc.parallelize(inventory1, 4).map(lambda x: x.lower())\n",
    "RDD2=sc.parallelize(inventory2, 4).map(lambda x: x.lower())\n",
    "print RDD1.collect()\n",
    "print RDD2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine both RDDs.\n",
    "####You can use [union()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=union#pyspark.RDD.union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDDall=RDD1.union(RDD2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Calculate ammounts of goods by using map() and reduceByKey(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairRDDall=RDDall.map(lambda x:(x,1)).reduceByKey(add)\n",
    "print pairRDDall.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Find out what goods are stored in warehouse 1 but not in warehouse 2. (you can use  [subtract(other, numPartitions=None)](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=subtract#pyspark.RDD.subtract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subtract RDD1 with RDD2\n",
    "# Create RDD pairs (goods, number of goods)\n",
    "# you can use .map and .reduceByKey\n",
    "\n",
    "diffRDD=RDD1.<FILL IN> # Use et least 2 functions\n",
    "\n",
    "diff = diffRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort diffRDD alphabeticaly (using [sortByKey() fucntion])(http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=sortbykey#pyspark.RDD.sortByKey) and print its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_sorted = diffRDD.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST \n",
    "\n",
    "Test.assertEqualsHashed(diffRDD.take(1), '280153e0c50e105f8d2380fc64fe4a2f162abfe9', 'Test 13 success')\n",
    "Test.assertEqualsHashed(diff_sorted[1], '5c9da466e4f3799ea3a9aa7950ef06b23db3fda4', 'Test 14 success')\n",
    "Test.assertEqualsHashed(diff_sorted, '89066489ead47cae2fa3737e8c4a82bc3e9afbd6', 'Test 15 success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Presented by <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"http://datascience-school.com\">datascience-school.com</a></h3></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
